# MyEyes: Real-Time Object Detection with AR Navigation for the Visually Impaired

**Author**: David Chung, Albert Luo, Richie Tran, Alvin Sung  
**Platform**: iOS (UIKit + ARKit) & Python (Flask + YOLOv8)  
**Role**: iOS Frontend + ARKit Integration (Swift)  
**Mentors**: Independently developed with backend model integration via REST APIs

## üß† Project Overview

**MyEyes** is an assistive iOS application that functions as an intelligent "visual narrator" for the visually impaired. Using **LiDAR**, **ARKit**, **YOLOv8**, and a real-time **Flask server**, the app:
- Detects and prioritizes real-world objects using the phone's camera.
- Calculates spatial depth to detect nearby obstructions.
- Audibly narrates surroundings and issues proximity-based alerts.

## üîç Key Features

### üß≠ Swift AR Navigation Interface (iOS UIKit)
- Leverages **ARKit + SceneDepth** to map 3D space.
- Tracks user movement via device motion and camera transform.
- Uses **AVSpeechSynthesizer** to narrate detected objects in the user‚Äôs path.
- Plays vibration + system sound alarm when nearby obstacle detected (`< 2.5 ft`).

### üì∑ Object Detection with YOLOv8 + Deep SORT (Python backend)
- Processes camera frames via a REST API and returns top-priority objects.
- Uses **YOLOv8** for detection + **DeepSORT** for multi-object tracking.
- Prioritizes objects ‚Äúahead‚Äù using bounding box centroids.
- Filters and ranks objects based on size, position, motion delta, and type.

### üë®‚Äçüíª My Contribution

This project was a collaborative effort. I, **David Chung**, was responsible for the development of the iOS application in Swift. My key contributions included:

- **Developed the entire front-end using Swift and UIKit**, including the camera interface, live image preview, and user interaction flow.
- **Integrated real-time communication with the Python Flask server** via HTTP requests for image analysis and voice response.
- **Implemented voice output using `AVSpeechSynthesizer`** to provide accessible spoken feedback for visually impaired users.
- **Engineered efficient view controller logic**, including asynchronous data handling and UI updates to ensure a seamless user experience.
- **Led system integration efforts**, ensuring smooth coordination between client-side (iOS) and server-side (Flask) components. - **Utilized LiDAR technology** for enhanced scene understanding and spatial context recognition.
- **Optimized image capture and transmission pipeline** to minimize latency and maximize reliability, especially in mobile network conditions.
- **Streamlined memory usage and CPU overhead** in the app by profiling key UI interactions and leveraging lazy loading where applicable.
- **Focused on clean architecture, code reusability, and responsiveness**, adhering to iOS best practices for accessibility and performance.
  
## üõ†Ô∏è Technical Stack

| Component | Technology |
|----------|------------|
| iOS Frontend | Swift, ARKit, AVFoundation, UIKit |
| Server Backend | Flask, OpenCV, Ultralytics YOLOv8 |
| AI Model | YOLOv8m, DeepSort |
| Communication | HTTP POST (image bytes via `URLSession`) |
| Deployment | Local server + mobile client (tested on-device) |


## üì¶ File Structure (Swift-side)

```
myEyes/
‚îú‚îÄ‚îÄ ViewController.swift       # ARKit scene logic, image capture, server calls, speech feedback
‚îú‚îÄ‚îÄ speakWords.swift           # Speech synthesis for detected object list
‚îú‚îÄ‚îÄ CurrentImage.swift         # Global image state (optional)
‚îú‚îÄ‚îÄ AppDelegate.swift          # Standard iOS app lifecycle
‚îú‚îÄ‚îÄ SceneDelegate.swift        # Scene routing setup
```


## üìà System Flow

```text
[User holds phone] ‚Üí [ARKit captures scene]
           ‚Üì
[Depth map analyzed for obstacle warning]
           ‚Üì
[Frame captured & sent to Python server]
           ‚Üì
[YOLOv8 detects objects ‚Üí ranks ‚Üí returns JSON]
           ‚Üì
[iOS reads top objects aloud & alerts user if needed]
```


## üöÄ How to Run

### iOS App (Swift)
1. Open `myEyes.xcodeproj` in Xcode.
2. Ensure you‚Äôre using a physical device (LiDAR supported).
3. Run the app.

### Python Server (Backend)
```bash
pip install -r requirements.txt
python sample_server.py
```
- Flask app listens on `http://<ip>:5058/detect`
- Accepts `POST` requests with JPEG image bytes.


## üß† Engineering Highlights

- ‚úÖ Real-time depth sensing using `ARSceneDepth` and optimized pixel buffer traversal.
- ‚úÖ Server communication throttled based on camera movement using `simd_distance()`.
- ‚úÖ Efficient Swift concurrency with non-blocking LiDAR parsing.
- ‚úÖ Fully modular object detection backend using YOLOv8 + Deep SORT.
- ‚úÖ Custom heap prioritization in Python for object importance ranking.
- ‚úÖ Speech synthesis with AVSpeechSynthesizer tuned for clarity and natural pacing.


## üí¨ Why It Matters

> MyEyes empowers visually impaired users to better understand their surroundings using only a mobile phone‚Äîturning passive vision into active guidance. It‚Äôs built with performance, accessibility, and real-world application in mind.


